{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is a guide for binary classification pipeline using Python, Pandas, Scikit-learn, Numpy and other libraries:\n",
    "\n",
    "1. Descriptive statistics\n",
    "   - Use Pandas to load the data and get a basic understanding of the data.\n",
    "   - Use functions such as `df.head()`, `df.info()`, `df.describe()`, and `df.shape` to check the structure, the types, and the distribution of data.\n",
    "\n",
    "2. Data cleaning and missing values replacement\n",
    "   - Identify and remove duplicates, if any.\n",
    "   - Check for missing values and handle them using techniques such as imputation or removal. \n",
    "   - Use Pandas' `fillna()` function to fill missing values with a specific value or a statistical measure such as the mean or median.\n",
    "\n",
    "3. Handling outliers\n",
    "   - Identify the outliers using techniques such as boxplots or scatterplots.\n",
    "   - Remove the outliers or apply a transformation such as log or square root to the data.\n",
    "\n",
    "4. Encoding categorical variables using onehot and target encodings\n",
    "   - Convert categorical variables to numerical values using one-hot encoding or target encoding. \n",
    "   - Use Scikit-learn's `OneHotEncoder` or `LabelEncoder` for one-hot encoding or target encoding.\n",
    "\n",
    "5. Feature selection using correlation \n",
    "   - Identify and remove features that are highly correlated with each other. \n",
    "   - Use Pandas' `corr()` function to check the correlation between features.\n",
    "   - Drop highly correlated features using `df.drop()` function.\n",
    "\n",
    "6. Numerical columns normalisation using min-max scaler\n",
    "   - Normalise the numerical columns using techniques such as min-max scaling.\n",
    "   - Use Scikit-learn's `MinMaxScaler` to scale the numerical data between 0 and 1.\n",
    "\n",
    "7. Modeling using logistic regression, decision tree, random forest and LGBM\n",
    "   - Split the data into training and testing sets using Scikit-learn's `train_test_split()`.\n",
    "   - Build the models using Scikit-learn's `LogisticRegression()`, `DecisionTreeClassifier()`, `RandomForestClassifier()`, and `LGBMClassifier()`.\n",
    "   - Fit the models using `fit()` function and predict using `predict()` function.\n",
    "\n",
    "8. Interpretation of results using f1, precision, recal, ROC-AUC and confusion matrix\n",
    "   - Evaluate the model performance using metrics such as f1-score, precision, recall, ROC-AUC curve, and confusion matrix.\n",
    "   - Use Scikit-learn's `classification_report()`, `roc_auc_score()`, and `confusion_matrix()` to evaluate the model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b> Dataset Description<b></span>\n",
    "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with over two hundred thousand observations and nearly 800 features.  Each observation is independent from the previous. \n",
    "\n",
    "For each observation, it was recorded whether a default was triggered. In case of a default, the loss was measured. This quantity lies between 0 and 100. It has been normalised, considering that the notional of each transaction at inception is 100. For example, a loss of 60 means that only 40 is reimbursed. If the loan did not default, the loss was 0. You are asked to predict the losses for each observation in the test set.\n",
    "\n",
    "Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Note that some variables may be categorical (e.g. f776 and f777).\n",
    "\n",
    "The competition sponsor has worked to remove time-dimensionality from the data. However, the observations are still listed in order from old to new in the training set. In the test set they are in random order.\n",
    "\n",
    "More info: https://www.kaggle.com/competitions/loan-default-prediction/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_v2.csv\")\n",
    "test_df = pd.read_csv(\"test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.select_dtypes(include=['object']).head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features seem invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical columns which are invalid and drop id columns in train and test data\n",
    "invalid = train_df.select_dtypes(include=['object']).columns\n",
    "train_df.drop(invalid, axis=1, inplace=True)\n",
    "test_df.drop(invalid, axis=1, inplace=True)\n",
    "train_df_id = train_df['id'].copy()\n",
    "train_df.drop('id', axis=1, inplace=True)\n",
    "test_df_id = test_df['id'].copy\n",
    "test_df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b> EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info(); test_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b> Missing vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_miss = train_df.isnull().sum()\n",
    "train_miss = pd.DataFrame(train_miss[train_miss > 0])\n",
    "train_miss.columns = ['Number_missing']\n",
    "train_miss['Percent_missing'] = train_miss['Number_missing'] / len(train_df) * 100\n",
    "train_miss.sort_values(by='Percent_missing', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.fillna(train_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.fillna(test_df.mean(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen\"><b> We can select the most informational features and drop the most correlated, e.g. setting thereshold > 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_for_corr = train_df['loss']\n",
    "train_df_for_corr= train_df.drop('loss', axis=1)\n",
    "correlations = train_df_for_corr.corr(method='spearman').abs()\n",
    "#correlations = correlations['loss'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_test = test_df.corr(method='spearman').abs()\n",
    "upper_test = correlations_test.where(np.triu(np.ones(correlations_test.shape), k=1).astype(np.bool))\n",
    "threshold = 0.90\n",
    "to_drop_test = [column for column in upper_test.columns if any(upper_test[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove in test df.' % (len(to_drop_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking upper triangular part of correlation matrix \n",
    "upper = correlations.where(np.triu(np.ones(correlations.shape), k=1).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with correlations above threshold\n",
    "threshold = 0.90\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not drop as we will apply PCA\n",
    "train_df = train_df.drop(columns = to_drop)\n",
    "test_df = test_df.drop(columns = to_drop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding correlation of features with target values of loss and convert into a dataframe\n",
    "corr_tar = train_df.corrwith(y).sort_values()\n",
    "print(corr_tar.head(10))\n",
    "print(corr_tar.tail(10))\n",
    "corr_tar_df = corr_tar.to_frame().transpose()\n",
    "corr_tar_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features having NaN value correlation with loss to remove them \n",
    "col_to_drop_1 = corr_tar_df.columns[corr_tar_df.isna().any()].to_list()\n",
    "print(len(col_to_drop_1))\n",
    "print(col_to_drop_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = col_to_drop_1)\n",
    "test_df = test_df.drop(columns = col_to_drop_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b> Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['loss']\n",
    "X = train_df.drop('loss', axis=1)\n",
    "#y = train_df['loss']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y>0] = 1\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train.shape, X_test.shape, y_train.shape, y_test.shape, test_df.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = set(X_train.columns) - set(test_df.columns)\n",
    "diff_cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b>Standardization\n",
    "PCA is effected by scale so we need to scale the features in the data before applying PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_df_scaled = scaler.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df_scaled = scaler.transform(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b> Applying PCA</span>\n",
    "PCA is a method used to reduce number of variables in the data by extracting the important ines from a large pool. It reduces the dimension of the data with an aim to retain as much information as possible. \n",
    "Method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called \" principal components\" that aacount for most variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)[200]\n",
    "print(f'{np.cumsum(pca.explained_variance_ratio_)[200]:.2f}' + ' of the variance is explained by 200 components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca = PCA(n_components=200)\n",
    "final_pca.fit(X_train_scaled)\n",
    "X_train_pca = final_pca.transform(X_train_scaled)\n",
    "X_test_pca = final_pca.transform(X_test_scaled)\n",
    "test_pca = final_pca.transform(test_df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape, test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pd.DataFrame(X_train_pca)\n",
    "X_test_pca = pd.DataFrame(X_test_pca)\n",
    "test_pca = pd.DataFrame(test_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:darkgreen\"><b>Use these variables to fit the model with 200 independent variables to predict loss.</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b>Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "logreg = LogisticRegression(solver = 'saga', class_weight= 'balanced', max_iter=500, random_state = 1)\n",
    "logreg.fit(X_train_pca, y_train)\n",
    "logreg.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = logreg.coef_[0]\n",
    "# summarize feature importance\n",
    "for i, v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i, v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "sns.barplot(x = 'weight', y = 'feature', data = pd.DataFrame({'feature': X_train_pca.columns, 'weight': logreg.coef_[0]}).sort_values(by = 'weight', ascending = False).iloc[0:50] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkgreen\"><b>Validation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test_pca)\n",
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen\"><b>Model Evaluation   Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "c = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred), index = \n",
    "                 [\"Actual non defaulter\", \n",
    "                \"Actual defaulter\"])\n",
    "c.columns = [\"Predicted non defaulter\", \"Predicted defaulter\"]\n",
    "c['Actual Total'] = c.sum(axis = 1)\n",
    "c.loc['Predicted Total', :] = c.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen\"><b>Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The accuracy on the validation data is '+ str(round(metrics.accuracy_score(y_test, y_pred)*100, ndigits=2)) +\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen\"><b>Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The sensitivity (true positive rate) is \"+ str(round(metrics.recall_score(y_test, y_pred)*100, ndigits = 2)) + \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen\"><b>AUC Area Under the Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_fpr, ns_tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "np_zero_fpr, np_zero_tpr, _ = metrics.roc_curve(y_test, np.zeros(len(y_test)))\n",
    "\n",
    "logreg_probs = logreg.predict_proba(X_test_pca)\n",
    "# keep probabilities for the positive outcome only\n",
    "logreg_probs = logreg_probs[:, 1]\n",
    "logreg_fpr, logreg_tpr, _ = metrics.roc_curve(y_test, logreg_probs)\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle = '--', label = 'Logistic Regression probability')\n",
    "plt.plot(np_zero_fpr, np_zero_tpr, linestyle = '--', label  = 'No prediction\"')\n",
    "plt.plot(logreg_fpr, logreg_tpr, marker = \".\", label = 'Logistic Regression')\n",
    "# axis labels\n",
    "plt.xlabel('False Posititve Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the lehend\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Area under ROC CURVE is ' +str(round(100*metrics.roc_auc_score(y_test, y_pred), ndigits = 2)) +'%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:darkgreen\"><b>Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkgreen\"><b>Prediction on given test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg.predict(test_pca)\n",
    "sns.countplot(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission = pd.DataFrame({'id': test_df_id, 'loss': pred})\n",
    "#submission.to_csv('submission.csv', index= False)\n",
    "submission = pd.read_csv('sampleSubmission.csv')\n",
    "submission['loss'] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(X_train_pca .shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances += logreg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({'feature': list(X_train_pca.columns), \n",
    "                                    'importance':feature_importances}).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to one-dimensional array (vector)\n",
    "y_train = np.array(y_train).reshape((-1, ))\n",
    "y_test = np.array(y_test).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Models to Evaluate\n",
    "\n",
    "# We will compare five different machine learning Cassification models:\n",
    "\n",
    "# 1 - Logistic Regression\n",
    "# 2 - K-Nearest Neighbors Classification\n",
    "# 3 - Suport Vector Machine\n",
    "# 4 - Naive Bayes\n",
    "# 5 - Random Forest Classification\n",
    "\n",
    "# Function to calculate mean absolute error\n",
    "def cross_val(X_train, y_train, model):\n",
    "    # Applying k-Fold Cross Validation\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n",
    "    return accuracies.mean()\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_cross = cross_val(X_train, y_train, model)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return model_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive = GaussianNB()\n",
    "naive_cross = fit_and_evaluate(naive)\n",
    "\n",
    "print('Naive Bayes Performance on the test set: Cross Validation Score = %0.4f' % naive_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "random_cross = fit_and_evaluate(random)\n",
    "\n",
    "print('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradiente Boosting Classification\n",
    "from xgboost import XGBClassifier\n",
    "gb = XGBClassifier()\n",
    "gb_cross = fit_and_evaluate(gb)\n",
    "\n",
    "print('Gradiente Boosting Classification Performance on the test set: Cross Validation Score = %0.4f' % gb_cross)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
